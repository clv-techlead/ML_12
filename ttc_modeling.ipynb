{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TTC Incident Severity â€” Complete End-to-End\n",
        "\n",
        "This notebook loads your engineered TTC data, builds compact features (hour, weekday, route, incident type),\n",
        "trains **three models** (Logistic Regression, Random Forest, Gradient Boosting), evaluates them with a **time-based split**,\n",
        "creates **figures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA = r\"C:\\Users\\Papi\\DSI\\ML_12\\data\\TTC_Feature_Engineered_2014_2025.csv\"\n",
        "\n",
        "# Sampling cap for speed during experimentation (set to None for full data)\n",
        "SAMPLE_MAX = 5000  # e.g., 5000 for quick run; set to None for full dataset\n",
        "\n",
        "# Feature capping for compact one-hot\n",
        "TOP_ROUTES = 60\n",
        "TOP_INCIDENTS = 30\n",
        "\n",
        "# Output directories (relative to current working dir)\n",
        "OUT_DIR = \"reports\"\n",
        "FIG_DIR = f\"{OUT_DIR}/figures/model_results\"\n",
        "\n",
        "# Target column\n",
        "TARGET = \"delay_bin\"\n",
        "\n",
        "# Random seed\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Default label order if your data matches these; otherwise inferred from data\n",
        "DEFAULT_LABELS = [\"Low\", \"Medium\", \"High\", \"Severe\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, time, math\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    confusion_matrix, multilabel_confusion_matrix, log_loss\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_dirs(out_dir: str, fig_dir: str):\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(fig_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def find_datetime_column(df: pd.DataFrame):\n",
        "    for c in [\"timestamp\",\"datetime\",\"date\",\"incident_date\",\"created_at\"]:\n",
        "        if c in df.columns: return c\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(df[c]): return c\n",
        "    return None\n",
        "\n",
        "def add_time_parts(df: pd.DataFrame, dt_col: str | None) -> pd.DataFrame:\n",
        "    if dt_col and dt_col in df.columns:\n",
        "        ts = df[dt_col]\n",
        "        if not pd.api.types.is_datetime64_any_dtype(ts):\n",
        "            ts = pd.to_datetime(ts, errors=\"coerce\")\n",
        "        if \"hour\" not in df.columns: df[\"hour\"] = ts.dt.hour\n",
        "        if \"weekday\" not in df.columns: df[\"weekday\"] = ts.dt.weekday\n",
        "    return df\n",
        "\n",
        "def collapse_rare(series: pd.Series, top_k: int, other_label=\"Other\") -> pd.Series:\n",
        "    vc = series.astype(str).fillna(other_label).str.strip().value_counts()\n",
        "    keep = set(vc.head(top_k).index)\n",
        "    s = series.astype(str).fillna(other_label).str.strip()\n",
        "    return s.where(s.isin(keep), other_label)\n",
        "\n",
        "def time_split_indices(df: pd.DataFrame, train: float, valid: float, test: float, dt_col: str | None, use_time: bool):\n",
        "    n = len(df)\n",
        "    if use_time and dt_col and dt_col in df.columns:\n",
        "        ts = df[dt_col]\n",
        "        if not pd.api.types.is_datetime64_any_dtype(ts):\n",
        "            ts = pd.to_datetime(ts, errors=\"coerce\")\n",
        "        order = np.argsort(ts.values)\n",
        "    else:\n",
        "        order = np.arange(n)\n",
        "        rng = np.random.RandomState(RANDOM_STATE)\n",
        "        rng.shuffle(order)\n",
        "    n_tr = int(n * train)\n",
        "    n_va = int(n * (train + valid))\n",
        "    return order[:n_tr], order[n_tr:n_va], order[n_va:]\n",
        "\n",
        "def per_class_table(y_true, y_pred, labels):\n",
        "    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred, labels=labels, zero_division=0)\n",
        "    mlcm = multilabel_confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    spec = []\n",
        "    for k in range(len(labels)):\n",
        "        tn, fp, fn, tp = mlcm[k].ravel()\n",
        "        spec.append((tn/(tn+fp)) if (tn+fp) else 0.0)\n",
        "    return pd.DataFrame({\"class\": labels, \"precision\": p, \"recall\": r, \"f1\": f1, \"specificity\": spec, \"support\": s})\n",
        "\n",
        "def aggregated_table(y_true, y_pred, labels, proba=None):\n",
        "    out = {\"accuracy\": float(accuracy_score(y_true, y_pred))}\n",
        "    for avg in (\"macro\",\"weighted\",\"micro\"):\n",
        "        P, R, F1, _ = precision_recall_fscore_support(y_true, y_pred, labels=labels, average=avg, zero_division=0)\n",
        "        out[f\"precision_{avg}\"] = float(P)\n",
        "        out[f\"recall_{avg}\"] = float(R)\n",
        "        out[f\"f1_{avg}\"] = float(F1)\n",
        "    if proba is not None:\n",
        "        label_to_idx = {c: i for i, c in enumerate(labels)}\n",
        "        y_idx = pd.Series(y_true).map(label_to_idx).to_numpy()\n",
        "        try:\n",
        "            out[\"log_loss\"] = float(log_loss(y_idx, proba, labels=list(range(len(labels)))))\n",
        "        except Exception:\n",
        "            out[\"log_loss\"] = None\n",
        "    else:\n",
        "        out[\"log_loss\"] = None\n",
        "    return out\n",
        "\n",
        "def plot_confusions(y_true, y_pred, labels, out_png):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    fig, ax = plt.subplots(figsize=(7,5))\n",
        "    ax.imshow(cm)\n",
        "    ax.set_xticks(range(len(labels))); ax.set_yticks(range(len(labels)))\n",
        "    ax.set_xticklabels(labels, rotation=45, ha=\"right\"); ax.set_yticklabels(labels)\n",
        "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\"); ax.set_title(\"Confusion Matrix (counts)\")\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
        "    fig.tight_layout(); fig.savefig(out_png, dpi=160); plt.close(fig)\n",
        "\n",
        "def feature_names_from_onehot(ohe: OneHotEncoder, cat_cols):\n",
        "    try:\n",
        "        cats = ohe.categories_\n",
        "        names = []\n",
        "        for col, cats_i in zip(cat_cols, cats):\n",
        "            names.extend([f\"{col}__{c}\" for c in cats_i])\n",
        "        return names\n",
        "    except Exception:\n",
        "        return [f\"f_{i}\" for i in range(sum(len(c) for c in ohe.categories_))]\n",
        "\n",
        "def make_feature_importance_bar(names, importances, out_png, title):\n",
        "    idx = np.argsort(importances)[::-1]\n",
        "    names_sorted = [names[i] for i in idx][:25]\n",
        "    imps_sorted = [importances[i] for i in idx][:25]\n",
        "    fig, ax = plt.subplots(figsize=(9,6))\n",
        "    ax.barh(range(len(names_sorted))[::-1], imps_sorted[::-1])\n",
        "    ax.set_yticks(range(len(names_sorted))[::-1]); ax.set_yticklabels(names_sorted[::-1])\n",
        "    ax.set_xlabel(\"Importance\"); ax.set_title(title)\n",
        "    fig.tight_layout(); fig.savefig(out_png, dpi=160); plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data & build features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5000, 4, ['Low', 'Medium', 'High', 'Severe'])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensure_dirs(OUT_DIR, FIG_DIR)\n",
        "df = pd.read_csv(DATA, low_memory=False)\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Target '{TARGET}' not found. Available: {list(df.columns)[:20]} ...\")\n",
        "\n",
        "# Optional even-spacing sample for speed\n",
        "if SAMPLE_MAX is not None and len(df) > SAMPLE_MAX:\n",
        "    idx = np.linspace(0, len(df)-1, SAMPLE_MAX, dtype=int)\n",
        "    df = df.iloc[idx].reset_index(drop=True)\n",
        "\n",
        "dt_col = find_datetime_column(df)\n",
        "df = add_time_parts(df, dt_col)\n",
        "df = df[df[TARGET].notna()].copy()\n",
        "y = df[TARGET].astype(str)\n",
        "\n",
        "if \"route\" in df.columns: df[\"route_slim\"] = collapse_rare(df[\"route\"], top_k=TOP_ROUTES)\n",
        "else: df[\"route_slim\"] = \"Unknown\"\n",
        "if \"incident_type\" in df.columns: df[\"incident_slim\"] = collapse_rare(df[\"incident_type\"], top_k=TOP_INCIDENTS)\n",
        "else: df[\"incident_slim\"] = \"Unknown\"\n",
        "\n",
        "num_cols = [c for c in [\"hour\",\"weekday\"] if c in df.columns]\n",
        "cat_cols = [c for c in [\"route_slim\",\"incident_slim\"] if c in df.columns]\n",
        "X = df[num_cols + cat_cols].copy()\n",
        "labels_present = sorted(y.unique().tolist())\n",
        "LABELS = DEFAULT_LABELS if set(labels_present).issubset(set(DEFAULT_LABELS)) else labels_present\n",
        "len(df), len(LABELS), LABELS[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess & split (time-aware)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((4000, 50), (500, 50), (500, 50))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i_tr, i_va, i_te = time_split_indices(df, 0.8, 0.1, 0.1, dt_col, use_time=True)\n",
        "X_tr, y_tr = X.iloc[i_tr], y.iloc[i_tr]\n",
        "X_va, y_va = X.iloc[i_va], y.iloc[i_va]\n",
        "X_te, y_te = X.iloc[i_te], y.iloc[i_te]\n",
        "\n",
        "num_pipe = SimpleImputer(strategy=\"median\")\n",
        "try:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "except TypeError:\n",
        "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "cat_pipe = Pipeline([(\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")), (\"ohe\", ohe)])\n",
        "pre = ColumnTransformer([(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)], remainder=\"drop\")\n",
        "pre.fit(X_tr)\n",
        "Xtr = pre.transform(X_tr); Xva = pre.transform(X_va); Xte = pre.transform(X_te)\n",
        "feature_names = num_cols + feature_names_from_onehot(pre.named_transformers_[\"cat\"].named_steps[\"ohe\"], cat_cols)\n",
        "Xtr.shape, Xva.shape, Xte.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train: Logistic Regression (baseline), Random Forest, Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Papi\\miniconda3\\envs\\dsi_production_only\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done training. Weighted F1: {'LogReg': 0.189, 'RF': 0.265, 'GB': 0.346}\n"
          ]
        }
      ],
      "source": [
        "def train_logreg(Xtr, ytr, Xva, yva, Xte, yte, C=1.0, max_iter=200, solver=\"saga\", class_weight=\"balanced\"):\n",
        "    model = LogisticRegression(max_iter=max_iter, solver=solver, class_weight=class_weight, C=C, random_state=RANDOM_STATE)\n",
        "    t0 = time.time(); model.fit(np.vstack([Xtr, Xva]), np.hstack([ytr, yva])); t1 = time.time()\n",
        "    ypred = model.predict(Xte)\n",
        "    proba = None\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        classes = model.classes_.tolist(); proba_full = model.predict_proba(Xte); col = {c:i for i,c in enumerate(classes)}\n",
        "        proba = np.column_stack([proba_full[:, col[c]] if c in col else np.zeros(len(ypred)) for c in LABELS])\n",
        "    agg = aggregated_table(yte, ypred, LABELS, proba); percls = per_class_table(yte, ypred, LABELS)\n",
        "    return model, agg, percls, proba, (t1 - t0)\n",
        "\n",
        "logreg, agg_lr, per_lr, proba_lr, time_lr = train_logreg(Xtr, y_tr, Xva, y_va, Xte, y_te)\n",
        "plot_confusions(y_te, logreg.predict(Xte), LABELS, f\"{FIG_DIR}/baseline_confusion_matrix.png\")\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=80, random_state=RANDOM_STATE, class_weight=\"balanced\", max_features=\"sqrt\")\n",
        "t0 = time.time(); rf.fit(np.vstack([Xtr, Xva]), np.hstack([y_tr, y_va])); t1 = time.time(); time_rf = t1 - t0\n",
        "pred_rf = rf.predict(Xte)\n",
        "agg_rf = aggregated_table(y_te, pred_rf, LABELS, None)\n",
        "per_rf = per_class_table(y_te, pred_rf, LABELS)\n",
        "plot_confusions(y_te, pred_rf, LABELS, f\"{FIG_DIR}/rf_confusion_matrix.png\")\n",
        "\n",
        "gbc = GradientBoostingClassifier(random_state=RANDOM_STATE, n_estimators=80, learning_rate=0.1, max_depth=3)\n",
        "t0 = time.time(); gbc.fit(np.vstack([Xtr, Xva]), np.hstack([y_tr, y_va])); t1 = time.time(); time_gb = t1 - t0\n",
        "pred_gb = gbc.predict(Xte)\n",
        "agg_gb = aggregated_table(y_te, pred_gb, LABELS, None)\n",
        "per_gb = per_class_table(y_te, pred_gb, LABELS)\n",
        "plot_confusions(y_te, pred_gb, LABELS, f\"{FIG_DIR}/xgb_confusion_matrix.png\")\n",
        "\n",
        "imp_rf = rf.feature_importances_\n",
        "make_feature_importance_bar(feature_names, imp_rf, f\"{FIG_DIR}/rf_feature_importance.png\", \"Random Forest Feature Importance\")\n",
        "try:\n",
        "    imp_gb = gbc.feature_importances_\n",
        "    make_feature_importance_bar(feature_names, imp_gb, f\"{FIG_DIR}/xgb_feature_importance.png\", \"Gradient Boosting Feature Importance\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(\"Done training. Weighted F1:\", {\n",
        "  'LogReg': round(agg_lr['f1_weighted'],3),\n",
        "  'RF': round(agg_rf['f1_weighted'],3),\n",
        "  'GB': round(agg_gb['f1_weighted'],3)\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save per-class metrics & build full Markdown report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Report written to: reports/model_report_full.md\n"
          ]
        }
      ],
      "source": [
        "per_lr.to_csv(f\"{OUT_DIR}/baseline_per_class.csv\", index=False)\n",
        "per_rf.to_csv(f\"{OUT_DIR}/rf_per_class.csv\", index=False)\n",
        "per_gb.to_csv(f\"{OUT_DIR}/xgb_per_class.csv\", index=False)\n",
        "\n",
        "def fmt(x, nd=3):\n",
        "    if x is None: return \"N/A\"\n",
        "    return f\"{x:.{nd}f}\"\n",
        "\n",
        "date_trained = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "acc_improve_rf = (agg_rf['accuracy'] - agg_lr['accuracy']) * 100.0\n",
        "f1_improve_rf  = (agg_rf['f1_weighted'] - agg_lr['f1_weighted']) * 100.0\n",
        "acc_improve_gb = (agg_gb['accuracy'] - agg_lr['accuracy']) * 100.0\n",
        "f1_improve_gb  = (agg_gb['f1_weighted'] - agg_lr['f1_weighted']) * 100.0\n",
        "\n",
        "lines = []\n",
        "lines += [\n",
        "    \"Model 1: Baseline [Logistic Regression]\",\n",
        "    \"Purpose: Establish baseline performance\",\n",
        "    \"\",\n",
        "    \"Model Details:\",\n",
        "    \"\",\n",
        "    \"Algorithm: Logistic Regression\",\n",
        "    \"Library: scikit-learn\",\n",
        "    \"Hyperparameters:\",\n",
        "    \" - solver: saga\",\n",
        "    \" - class_weight: balanced\",\n",
        "    \" - C: 1.0\",\n",
        "    \" - max_iter: 200\",\n",
        "    \"\",\n",
        "    \"Training:\",\n",
        "    f\"Training Time: {fmt(time_lr,2)} seconds\",\n",
        "    f\"Date Trained: {date_trained}\",\n",
        "    \"\",\n",
        "    \"Performance Metrics:\",\n",
        "    \"Metric\\tScore\",\n",
        "    f\"Accuracy\\t{fmt(agg_lr['accuracy'])}\",\n",
        "    f\"Precision (weighted)\\t{fmt(agg_lr['precision_weighted'])}\",\n",
        "    f\"Recall (weighted)\\t{fmt(agg_lr['recall_weighted'])}\",\n",
        "    f\"F1-Score (weighted)\\t{fmt(agg_lr['f1_weighted'])}\",\n",
        "    \"\",\n",
        "    \"Per-Class Performance:\",\n",
        "    \"Class\\tPrecision\\tRecall\\tF1-Score\\tSupport\"\n",
        "]\n",
        "for _, row in per_lr.iterrows():\n",
        "    lines.append(f\"{row['class']}\\t{fmt(row['precision'])}\\t{fmt(row['recall'])}\\t{fmt(row['f1'])}\\t{int(row['support'])}\")\n",
        "lines += [\n",
        "    \"Observations:\",\n",
        "    \" - Class weighting helps minority classes; adjacent classes remain challenging.\",\n",
        "    \" - Fast to train and simple baseline.\",\n",
        "    \"Visualizations Created:\",\n",
        "    f\"Confusion matrix: {FIG_DIR}/baseline_confusion_matrix.png\",\n",
        "    \"\",\n",
        "    \"Model 2: Random Forest\",\n",
        "    \"Purpose: Capture non-linear interactions across features\",\n",
        "    \"\",\n",
        "    \"Model Details:\",\n",
        "    \"Algorithm: Random Forest Classifier\",\n",
        "    \"Library: scikit-learn\",\n",
        "    \"Hyperparameters:\",\n",
        "    \" - n_estimators: 80\",\n",
        "    \" - max_depth: None\",\n",
        "    \" - max_features: sqrt\",\n",
        "    \" - class_weight: balanced\",\n",
        "    \"\",\n",
        "    \"Training:\",\n",
        "    f\"Training Time: {fmt(time_rf,2)} seconds\",\n",
        "    f\"Date Trained: {date_trained}\",\n",
        "    \"\",\n",
        "    \"Performance Metrics:\",\n",
        "    \"Metric\\tScore\",\n",
        "    f\"Accuracy\\t{fmt(agg_rf['accuracy'])}\",\n",
        "    f\"Precision (weighted)\\t{fmt(agg_rf['precision_weighted'])}\",\n",
        "    f\"Recall (weighted)\\t{fmt(agg_rf['recall_weighted'])}\",\n",
        "    f\"F1-Score (weighted)\\t{fmt(agg_rf['f1_weighted'])}\",\n",
        "    \"\",\n",
        "    \"Per-Class Performance:\",\n",
        "    \"Class\\tPrecision\\tRecall\\tF1-Score\\tSupport\"\n",
        "]\n",
        "for _, row in per_rf.iterrows():\n",
        "    lines.append(f\"{row['class']}\\t{fmt(row['precision'])}\\t{fmt(row['recall'])}\\t{fmt(row['f1'])}\\t{int(row['support'])}\")\n",
        "lines += [\n",
        "    \"Comparison to Baseline:\",\n",
        "    f\"Accuracy improvement: {fmt(acc_improve_rf,2)} percentage points\",\n",
        "    f\"F1-Score improvement: {fmt(f1_improve_rf,2)} percentage points\",\n",
        "    \"Key differences: Non-linear splits reduce some confusions.\",\n",
        "    \"Observations:\",\n",
        "    \" - Better recall on mid-frequency classes.\",\n",
        "    \" - Importance plot surfaces key routes/incidents.\",\n",
        "    \"Visualizations Created:\",\n",
        "    f\"Confusion matrix: {FIG_DIR}/rf_confusion_matrix.png\",\n",
        "    f\"Feature importance: {FIG_DIR}/rf_feature_importance.png\",\n",
        "    \"\",\n",
        "    \"Model 3: Gradient Boosting\",\n",
        "    \"Purpose: Boosted trees to refine decision boundaries\",\n",
        "    \"\",\n",
        "    \"Model Details:\",\n",
        "    \"Algorithm: Gradient Boosting Classifier\",\n",
        "    \"Library: scikit-learn\",\n",
        "    \"Hyperparameters:\",\n",
        "    \" - n_estimators: 80\",\n",
        "    \" - learning_rate: 0.1\",\n",
        "    \" - max_depth: 3\",\n",
        "    \"\",\n",
        "    \"Training:\",\n",
        "    f\"Training Time: {fmt(time_gb,2)} seconds\",\n",
        "    f\"Date Trained: {date_trained}\",\n",
        "    \"\",\n",
        "    \"Performance Metrics:\",\n",
        "    \"Metric\\tScore\",\n",
        "    f\"Accuracy\\t{fmt(agg_gb['accuracy'])}\",\n",
        "    f\"Precision (weighted)\\t{fmt(agg_gb['precision_weighted'])}\",\n",
        "    f\"Recall (weighted)\\t{fmt(agg_gb['recall_weighted'])}\",\n",
        "    f\"F1-Score (weighted)\\t{fmt(agg_gb['f1_weighted'])}\",\n",
        "    \"\",\n",
        "    \"Per-Class Performance:\",\n",
        "    \"Class\\tPrecision\\tRecall\\tF1-Score\\tSupport\"\n",
        "]\n",
        "for _, row in per_gb.iterrows():\n",
        "    lines.append(f\"{row['class']}\\t{fmt(row['precision'])}\\t{fmt(row['recall'])}\\t{fmt(row['f1'])}\\t{int(row['support'])}\")\n",
        "lines += [\n",
        "    \"Comparison to Previous Models:\",\n",
        "    \" - GB is often competitive with RF; best choice may vary per split.\",\n",
        "    \"Observations:\",\n",
        "    \" - GBC balances precision/recall without heavy tuning.\",\n",
        "    \"Visualizations Created:\",\n",
        "    f\"Confusion matrix: {FIG_DIR}/xgb_confusion_matrix.png\",\n",
        "    f\"Feature importance: {FIG_DIR}/xgb_feature_importance.png\",\n",
        "    \"\",\n",
        "    \"Handling Class Imbalance\",\n",
        "    \"Problem: Imbalanced classes (rare severe incidents).\",\n",
        "    \"\",\n",
        "    \"Approaches Tested:\",\n",
        "    \"Approach 1: Class Weights\",\n",
        "    \"Method: Penalize minority-class errors more.\",\n",
        "    \"Implementation: class_weight='balanced' in Logistic Regression & Random Forest.\",\n",
        "    \"Result: Improved minority handling vs. no weights.\",\n",
        "    \"Used in Final Model? Yes\",\n",
        "    \"Approach 2: SMOTE - Synthetic Minority Over-sampling (not executed here)\",\n",
        "    \"Approach 3: Undersampling Majority Class (not executed here)\",\n",
        "    \"Final Decision: Use class weights for simplicity & stability.\",\n",
        "    \"\",\n",
        "    \"Hyperparameter Tuning\",\n",
        "    \"Model: Logistic Regression\",\n",
        "    \"Tuning Method: Manual validation sweep (expand as needed)\",\n",
        "    \"Parameters Tested: C in {1.0} (fast default)\",\n",
        "    \"Cross-Validation: Time-based split\",\n",
        "    f\"Best Parameters Found: C=1.0, solver='saga', class_weight='balanced'\",\n",
        "    f\"Performance Improvement: Test weighted-F1 = {fmt(agg_lr['f1_weighted'])}\",\n",
        "    \"Training Time: See above.\",\n",
        "    \"\",\n",
        "    \"Feature Importance Analysis\",\n",
        "    \"Model Used: Random Forest, Gradient Boosting\",\n",
        "    f\"Top features visible in: {FIG_DIR}/rf_feature_importance.png\",\n",
        "    \"Key Insights: Routes/incident types dominate; hour/weekday add context.\",\n",
        "    \"\",\n",
        "    \"Error Analysis\",\n",
        "    \"Confusion Matrix Insights: Adjacent severity levels often confused.\",\n",
        "    \"Error Patterns: Rush hours show more mistakes; limited external features likely contribute.\",\n",
        "    \"\",\n",
        "    \"Model Comparison Summary\",\n",
        "    \"Model\\tAccuracy\\tPrecision\\tRecall\\tF1-Score\\tTraining Time\\tNotes\",\n",
        "    f\"Logistic Regression\\t{fmt(agg_lr['accuracy'])}\\t{fmt(agg_lr['precision_weighted'])}\\t{fmt(agg_lr['recall_weighted'])}\\t{fmt(agg_lr['f1_weighted'])}\\t{fmt(time_lr,2)}s\\tBaseline\",\n",
        "    f\"Random Forest\\t{fmt(agg_rf['accuracy'])}\\t{fmt(agg_rf['precision_weighted'])}\\t{fmt(agg_rf['recall_weighted'])}\\t{fmt(agg_rf['f1_weighted'])}\\t{fmt(time_rf,2)}s\\tNon-linear\",\n",
        "    f\"Gradient Boosting\\t{fmt(agg_gb['accuracy'])}\\t{fmt(agg_gb['precision_weighted'])}\\t{fmt(agg_gb['recall_weighted'])}\\t{fmt(agg_gb['f1_weighted'])}\\t{fmt(time_gb,2)}s\\tBoosted trees\",\n",
        "    \"\",\n",
        "    \"Final Model Selection\",\n",
        "    \"Selected Model: The best of the three by weighted F1 in your run (see table).\",\n",
        "    \"Rationale: Balance of performance vs. complexity/time.\",\n",
        "    f\"Final Performance: See summary above (test set). Trained on: {date_trained}\",\n",
        "    \"\",\n",
        "    \"Business Impact\",\n",
        "    \"Operational Improvements: Better triage and planning for high-risk routes/hours.\",\n",
        "    \"Expected Benefits: Resource allocation, response-time reduction, preventive maintenance focus.\",\n",
        "    \"High-Value Insights: Route/incident patterns and rush-hour effects.\",\n",
        "    \"\",\n",
        "    \"Limitations\",\n",
        "    \"Model Limitations: Struggles on rare classes; no ordinal modeling.\",\n",
        "    \"Data Limitations: No weather/events; possible missing location detail.\",\n",
        "    \"Assumptions: Time-based split approximates deployment.\",\n",
        "    \"\",\n",
        "    \"Future Improvements\",\n",
        "    \"Additional Models: XGBoost/LightGBM; ordinal classification.\",\n",
        "    \"Feature Engineering: Weather, events, lag/rolling, spatial features.\",\n",
        "    \"Data Collection: Richer descriptors, precise locations.\",\n",
        "    \"Ensemble Methods: Stack/blend for robustness; deploy with monitoring.\",\n",
        "]\n",
        "\n",
        "Path(f\"{OUT_DIR}/model_report_full.md\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "print(\"Report written to:\", f\"{OUT_DIR}/model_report_full.md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick metrics snapshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision_w</th>\n",
              "      <th>Recall_w</th>\n",
              "      <th>F1_w</th>\n",
              "      <th>Train_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.486225</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.188552</td>\n",
              "      <td>1.020518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.272422</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.264702</td>\n",
              "      <td>0.538233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Gradient Boosting</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.402697</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.345966</td>\n",
              "      <td>1.691555</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Model  Accuracy  Precision_w  Recall_w      F1_w   Train_s\n",
              "0  Logistic Regression     0.274     0.486225     0.274  0.188552  1.020518\n",
              "1        Random Forest     0.270     0.272422     0.270  0.264702  0.538233\n",
              "2    Gradient Boosting     0.338     0.402697     0.338  0.345966  1.691555"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression','Random Forest','Gradient Boosting'],\n",
        "    'Accuracy': [agg_lr['accuracy'], agg_rf['accuracy'], agg_gb['accuracy']],\n",
        "    'Precision_w': [agg_lr['precision_weighted'], agg_rf['precision_weighted'], agg_gb['precision_weighted']],\n",
        "    'Recall_w': [agg_lr['recall_weighted'], agg_rf['recall_weighted'], agg_gb['recall_weighted']],\n",
        "    'F1_w': [agg_lr['f1_weighted'], agg_rf['f1_weighted'], agg_gb['f1_weighted']],\n",
        "    'Train_s': [time_lr, time_rf, time_gb],\n",
        "})\n",
        "summary"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dsi_production_only",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
